{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Who Got Hepatitis C"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Packages for classifier\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "import math \n",
    "import random \n",
    "import csv\n",
    "\n",
    "# Packages for GLM\n",
    "from sklearn import preprocessing\n",
    "from sklearn import linear_model\n",
    "from sklearn import metrics\n",
    "import seaborn as sns\n",
    "sns.set(style='white')\n",
    "sns.set(style='whitegrid', color_codes=True)\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import statsmodels.api as sm\n",
    "import scipy.stats as stats\n",
    "from scipy.special import gammaln\n",
    "import itertools\n",
    "from itertools import chain\n",
    "matplotlib.rcParams['font.size'] = 18\n",
    "\n",
    "import time"
   ]
  },
  {
   "source": [
    "# 1. Naive Bayes Classifier"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def separate_by_class(dataset):\n",
    "    separated = {}\n",
    "    for i in range(len(dataset)):\n",
    "        vector = dataset[i]\n",
    "        if (vector[-1] not in separated):\n",
    "            separated[vector[-1]] = []\n",
    "        separated[vector[-1]].append(vector)\n",
    "    return separated\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to calculate mean values\n",
    "def mean(numbers):\n",
    "    return sum(numbers)/len(numbers)\n",
    "\n",
    "# function to calculate the sample standard deviation\n",
    "def stdev(numbers):\n",
    "    avg = mean(numbers)\n",
    "    variance = np.var(numbers)*(len(numbers))/(len(numbers) - 1)\n",
    "    return math.sqrt(variance)\n",
    "\n",
    "# calculate the mean and ssd of attributes in one class/group\n",
    "def summarize(dataset):\n",
    "    summaries = [(mean(attribute), stdev(attribute)) for attribute in  zip(*dataset)]\n",
    "    del summaries[-1]\n",
    "    return summaries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for every class/group, calculate the mean and standard deviation\n",
    "def summarize_by_class(dataset):\n",
    "    separated = separate_by_class(dataset)\n",
    "    summaries = {}\n",
    "    keyList = list(separated.keys())\n",
    "    for classValue in keyList:\n",
    "        summaries[classValue] = summarize(separated[classValue])\n",
    "    return summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the pdf of Normal dist to calculate the probability\n",
    "def calculate_probability(x, mean, stdev):\n",
    "    exponent = math.exp(-(math.pow(x-mean,2)/(2*math.pow(stdev,2))))\n",
    "    return (1 / (math.sqrt(2*math.pi) * stdev)) * exponent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the probability of every class/group\n",
    "def calculate_class_probabilities(summaries, inputVector):\n",
    "    probabilities = {}\n",
    "    keyList = list(summaries.keys())\n",
    "    for classValue in keyList:\n",
    "        probabilities[classValue] = 1\n",
    "        for i in range(len(summaries[classValue])):  # number of attributes\n",
    "            mean, stdev = summaries[classValue][i]   # The ith attributes got from training\n",
    "            x = inputVector[i]                       # the ith attributes from sample\n",
    "            probabilities[classValue] *= calculate_probability(x, mean, stdev)\n",
    "    return probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(summaries, inputVector):\n",
    "    probabilities = calculate_class_probabilities(summaries, inputVector)\n",
    "    bestLabel, bestProb = None, -1\n",
    "    # extract the label (class)\n",
    "    keyList = list(probabilities.keys())\n",
    "    # compare the probabilities of classes and select the best one\n",
    "    for classValue in keyList:\n",
    "        if bestLabel is None or probabilities[classValue] > bestProb:\n",
    "            bestProb = probabilities[classValue]\n",
    "            bestLabel = classValue\n",
    "    return bestLabel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions(summaries, testSet):\n",
    "    # for every single row data, get the best label and record\n",
    "    predictions = []\n",
    "    for i in range(len(testSet)):\n",
    "        result = predict(summaries, testSet[i])\n",
    "        predictions.append(result)\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy(testSet, predictions):\n",
    "    # number of successful predictions divided by the total observations\n",
    "    correct = 0\n",
    "    for x in range(len(testSet)):\n",
    "        if testSet[x][-1] == predictions[x]:\n",
    "            correct += 1\n",
    "    return (correct/float(len(testSet)))*100.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # import data\n",
    "    filename = 'hcvM.csv'\n",
    "    dataset = pd.read_csv(filename, header=None)\n",
    "    dataset = np.array(dataset)\n",
    " \n",
    "    # randomly divide the dataset into trainSet and testSet\n",
    "    trainSize = int(len(dataset)*2/3) \n",
    "    randomIdx = [i for i in range(len(dataset))]\n",
    "    random.shuffle(randomIdx)\n",
    "    trainSet = []\n",
    "    testSet = []\n",
    "    trainSet.extend(dataset[idx,:] for idx in randomIdx[:trainSize])\n",
    "    testSet.extend(dataset[idx,:] for idx in randomIdx[trainSize:])\n",
    " \n",
    "    # compute the mdoel\n",
    "    summaries = summarize_by_class(trainSet)\n",
    "    #print(summaries)\n",
    " \n",
    "    # test the model by testSet\n",
    "    predictions = get_predictions(summaries, testSet)\n",
    "    accuracy = get_accuracy(testSet, predictions)\n",
    "    print(('Accuracy:{0}%').format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Accuracy:93.81443298969072%\nThat took 0.012001514434814453 seconds\n"
     ]
    }
   ],
   "source": [
    "starttime = time.time()\n",
    "if __name__ == '__main__':\n",
    "    main()\n",
    "\n",
    "print('That took {} seconds'.format(time.time() - starttime))"
   ]
  },
  {
   "source": [
    "# 2. GLM Regression (Logit)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitset(data):\n",
    "    # extract the header\n",
    "    headers = data.columns.values\n",
    "    dataset = np.array(data)\n",
    "\n",
    "    # Randomly shuffle the data rows and split into two sets\n",
    "    trainSize = int(len(dataset)*2/3) \n",
    "    randomIdx = [i for i in range(len(dataset))]\n",
    "    random.shuffle(randomIdx)\n",
    "    trainSet = []\n",
    "    testSet = []\n",
    "    trainSet.extend(dataset[idx,:] for idx in randomIdx[:trainSize])\n",
    "    testSet.extend(dataset[idx,:] for idx in randomIdx[trainSize:])\n",
    "\n",
    "    trainSet = pd.DataFrame(trainSet, columns=headers)\n",
    "    testSet = pd.DataFrame(testSet, columns=headers)\n",
    "    return trainSet, testSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.9536082474226805\nThat took 0.025022268295288086 seconds\n"
     ]
    }
   ],
   "source": [
    "starttime = time.time()\n",
    "\n",
    "# data import\n",
    "hcvR = pd.read_csv('hcvR.csv')\n",
    "train_hcv, test_hcv = splitset(hcvR)\n",
    "\n",
    "Ty = train_hcv[['Cate']]\n",
    "Tx = train_hcv[['Age', 'ALB', 'ALP', 'ALT', 'AST', 'BIL', 'CHE', 'CHOL', 'CREA', 'GGT', 'PROT', 'SEX']]\n",
    "Sy = test_hcv[['Cate']]\n",
    "Sx = test_hcv[['Age', 'ALB', 'ALP', 'ALT', 'AST', 'BIL', 'CHE', 'CHOL', 'CREA', 'GGT', 'PROT', 'SEX']]\n",
    "\n",
    "dataT = {'X': np.array(Tx), 'y': np.array(Ty)}\n",
    "XT = np.insert(dataT['X'],0, values=np.ones(dataT['X'].shape[0]), axis=1)\n",
    "dataS = {'X': np.array(Sx), 'y': np.array(Sy)}\n",
    "XS = np.insert(dataS['X'],0, values=np.ones(dataS['X'].shape[0]), axis=1)\n",
    "\n",
    "# OvR approach\n",
    "lr = linear_model.LogisticRegression().fit(XT, dataT['y'])\n",
    "print(metrics.accuracy_score(dataS['y'], lr.predict(XS)))\n",
    "\n",
    "print('That took {} seconds'.format(time.time() - starttime))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.9639175257731959\nThat took 0.19217467308044434 seconds\n"
     ]
    }
   ],
   "source": [
    "starttime = time.time()\n",
    "\n",
    "# data import\n",
    "hcvR = pd.read_csv('hcvR.csv')\n",
    "train_hcv, test_hcv = splitset(hcvR)\n",
    "\n",
    "Ty = train_hcv[['Cate']]\n",
    "Tx = train_hcv[['Age', 'ALB', 'ALP', 'ALT', 'AST', 'BIL', 'CHE', 'CHOL', 'CREA', 'GGT', 'PROT', 'SEX']]\n",
    "Sy = test_hcv[['Cate']]\n",
    "Sx = test_hcv[['Age', 'ALB', 'ALP', 'ALT', 'AST', 'BIL', 'CHE', 'CHOL', 'CREA', 'GGT', 'PROT', 'SEX']]\n",
    "\n",
    "dataT = {'X': np.array(Tx), 'y': np.array(Ty)}\n",
    "XT = np.insert(dataT['X'],0, values=np.ones(dataT['X'].shape[0]), axis=1)\n",
    "dataS = {'X': np.array(Sx), 'y': np.array(Sy)}\n",
    "XS = np.insert(dataS['X'],0, values=np.ones(dataS['X'].shape[0]), axis=1)\n",
    "\n",
    "# MvM approach\n",
    "multi_lr = linear_model.LogisticRegression(multi_class='multinomial', solver='newton-cg').fit(XT, dataT['y'])\n",
    "print(metrics.accuracy_score(dataS['y'], multi_lr.predict(XS)))\n",
    "\n",
    "print('That took {} seconds'.format(time.time() - starttime))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}